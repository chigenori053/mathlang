{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MathLang Pro - Causal Analysis Introduction\n",
                "\n",
                "This notebook demonstrates advanced MathLang features for causal analysis and research.\n",
                "\n",
                "## Topics Covered\n",
                "- Counterfactual reasoning with CounterfactualNode\n",
                "- Advanced CoreRuntime configuration\n",
                "- Learning analytics and log analysis\n",
                "- Custom engine integration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import sys\n",
                "sys.path.insert(0, '../..')\n",
                "\n",
                "from core.parser import Parser\n",
                "from core.evaluator import Evaluator, SymbolicEvaluationEngine\n",
                "from core.symbolic_engine import SymbolicEngine\n",
                "from core.learning_logger import LearningLogger\n",
                "from core.computation_engine import ComputationEngine\n",
                "from core.validation_engine import ValidationEngine\n",
                "from core.hint_engine import HintEngine\n",
                "from core.core_runtime import CoreRuntime\n",
                "from core.exercise_spec import ExerciseSpec"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Counterfactual Reasoning\n",
                "\n",
                "The `Counterfactual` block allows you to explore \"what if\" scenarios in mathematical problem-solving."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Exploring alternative solution paths\n",
                "source = \"\"\"\n",
                "Problem: x**2 + 5*x + 6\n",
                "\n",
                "# Actual factorization\n",
                "Step: (x + 2)*(x + 3)\n",
                "\n",
                "# Counterfactual: What if we had x^2 + 5x + 4 instead?\n",
                "Counterfactual:\n",
                "  assume a = x**2 + 5*x + 4\n",
                "  expect (x + 1)*(x + 4)\n",
                "\n",
                "End: done\n",
                "\"\"\"\n",
                "\n",
                "parser = Parser()\n",
                "program = parser.parse(source)\n",
                "\n",
                "symbolic_engine = SymbolicEngine()\n",
                "engine = SymbolicEvaluationEngine(symbolic_engine)\n",
                "logger = LearningLogger()\n",
                "\n",
                "evaluator = Evaluator(program, engine, logger)\n",
                "success = evaluator.run()\n",
                "\n",
                "# Analyze counterfactual reasoning\n",
                "cf_records = [r for r in logger.records if r['phase'] == 'counterfactual']\n",
                "print(\"Counterfactual Analysis:\")\n",
                "for record in cf_records:\n",
                "    print(f\"  {record['rendered']}\")\n",
                "    if 'meta' in record and 'result' in record['meta']:\n",
                "        print(f\"  Result: {record['meta']['result']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. CoreRuntime Configuration\n",
                "\n",
                "For research purposes, you can configure CoreRuntime with custom engines and exercise specifications."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a research exercise with detailed tracking\n",
                "research_spec = ExerciseSpec(\n",
                "    id=\"research_001\",\n",
                "    target_expression=\"(x - 1)*(x + 1)\",\n",
                "    validation_mode=\"symbolic_equiv\",\n",
                "    intermediate_steps=[\n",
                "        \"x**2 - 1\",\n",
                "        \"(x - 1)*(x + 1)\"\n",
                "    ],\n",
                "    hint_rules={\n",
                "        \"x**2 + 1\": \"Sign error detected\",\n",
                "        \"x**2 - 2\": \"Constant error\"\n",
                "    },\n",
                "    metadata={\n",
                "        \"difficulty\": \"medium\",\n",
                "        \"concept\": \"difference_of_squares\",\n",
                "        \"study_id\": \"pilot_2024\"\n",
                "    }\n",
                ")\n",
                "\n",
                "# Initialize runtime\n",
                "symbolic = SymbolicEngine()\n",
                "computation = ComputationEngine(symbolic)\n",
                "validation = ValidationEngine(computation)\n",
                "hint = HintEngine(computation)\n",
                "logger = LearningLogger()\n",
                "\n",
                "runtime = CoreRuntime(computation, validation, hint, research_spec, logger)\n",
                "\n",
                "print(f\"RuntimeConfiguration:\")\n",
                "print(f\"  Exercise ID: {research_spec.id}\")\n",
                "print(f\"  Validation Mode: {research_spec.validation_mode}\")\n",
                "print(f\"  Study: {research_spec.metadata['study_id']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Simulating Student Responses\n",
                "\n",
                "For research, you can simulate various student response patterns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate different error patterns\n",
                "student_attempts = [\n",
                "    {\"answer\": \"x**2 + 1\", \"pattern\": \"sign_error\"},\n",
                "    {\"answer\": \"x**2 - 2\", \"pattern\": \"constant_error\"},\n",
                "    {\"answer\": \"x**2 - 1\", \"pattern\": \"expanded_form\"},\n",
                "    {\"answer\": \"(x - 1)*(x + 1)\", \"pattern\": \"correct\"}\n",
                "]\n",
                "\n",
                "results = []\n",
                "for attempt in student_attempts:\n",
                "    runtime.set(\"(x - 1)*(x + 1)\")\n",
                "    result = runtime.finalize(attempt[\"answer\"])\n",
                "    \n",
                "    results.append({\n",
                "        \"answer\": attempt[\"answer\"],\n",
                "        \"pattern\": attempt[\"pattern\"],\n",
                "        \"valid\": result[\"valid\"],\n",
                "        \"hint\": result[\"details\"].get(\"hint\", {}).get(\"message\") if not result[\"valid\"] else None\n",
                "    })\n",
                "\n",
                "# Analyze patterns\n",
                "import pandas as pd\n",
                "df = pd.DataFrame(results)\n",
                "print(\"\\nStudent Response Analysis:\")\n",
                "print(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Learning Analytics\n",
                "\n",
                "Extract insights from learning logs for analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run a full problem-solving session\n",
                "source = \"\"\"\n",
                "Problem: 2*x + 3*x\n",
                "Step: 5*x\n",
                "End: done\n",
                "\"\"\"\n",
                "\n",
                "parser = Parser()\n",
                "program = parser.parse(source)\n",
                "engine = SymbolicEvaluationEngine(SymbolicEngine())\n",
                "session_logger = LearningLogger()\n",
                "\n",
                "evaluator = Evaluator(program, engine, session_logger)\n",
                "evaluator.run()\n",
                "\n",
                "# Extract analytics\n",
                "log_df = pd.DataFrame(session_logger.records)\n",
                "\n",
                "print(\"Learning Log Summary:\")\n",
                "print(f\"  Total steps: {len(log_df)}\")\n",
                "print(f\"  Phases: {log_df['phase'].value_counts().to_dict()}\")\n",
                "print(f\"  Status breakdown: {log_df['status'].value_counts().to_dict()}\")\n",
                "\n",
                "# Show detailed logs\n",
                "print(\"\\nDetailed Log:\")\n",
                "print(log_df[['phase', 'expression', 'status', 'rule_id']])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Custom Analysis Functions\n",
                "\n",
                "Create custom analysis functions for your research."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_error_patterns(logs):\n",
                "    \"\"\"Analyze common error patterns in logs.\"\"\"\n",
                "    errors = [log for log in logs if log['status'] == 'mistake']\n",
                "    \n",
                "    patterns = {}\n",
                "    for error in errors:\n",
                "        reason = error.get('meta', {}).get('reason', 'unknown')\n",
                "        patterns[reason] = patterns.get(reason, 0) + 1\n",
                "    \n",
                "    return patterns\n",
                "\n",
                "def calculate_success_rate(logs):\n",
                "    \"\"\"Calculate step success rate.\"\"\"\n",
                "    steps = [log for log in logs if log['phase'] in ['step', 'end']]\n",
                "    if not steps:\n",
                "        return 0.0\n",
                "    \n",
                "    successes = len([s for s in steps if s['status'] == 'ok'])\n",
                "    return successes / len(steps)\n",
                "\n",
                "# Example usage\n",
                "patterns = analyze_error_patterns(session_logger.records)\n",
                "success_rate = calculate_success_rate(session_logger.records)\n",
                "\n",
                "print(f\"\\nError Patterns: {patterns}\")\n",
                "print(f\"Success Rate: {success_rate:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "- Explore `pro_advanced_analysis.ipynb` for log mining techniques\n",
                "- Check documentation for custom engine development\n",
                "- See examples in `pro/examples/` for real research applications"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}